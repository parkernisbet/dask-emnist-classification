{
 "cells": [
  {
   "cell_type": "raw",
   "id": "811bfbd3",
   "metadata": {
    "active": "py"
   },
   "source": [
    "!/usr/bin/env python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f495547",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "# Dask EMNIST Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a21bac",
   "metadata": {
    "cell_marker": "''','''",
    "region_name": "md"
   },
   "source": [
    "This notebook explores handwritten character classification using \n",
    "Dask-parallelized support vector machines. The dataset was sourced from \n",
    "[Kaggle](https://www.kaggle.com/vaibhao/handwritten-characters) and is a \n",
    "semi-subset of the more well known [Extended MNIST]\n",
    "(https://www.nist.gov/itl/products-and-services/emnist-dataset) \n",
    "(EMNIST) database. It includes just north of 850,000 handwritten digits, spread \n",
    "across 39 unique characters: all 26 capitalized English alphabet letters \n",
    "(A - Z), 9 real numbers (1 - 9), and 4 special characters (@, #, $, &). Note \n",
    "that this dataset's author merged the two categories 'O' (letter) and '0' \n",
    "(number) to reduce misclassifiations. The images have already been divided into \n",
    "train and validation folders, each containing subdirectories for all of the \n",
    "above mentioned 39 characters. In contrast to our prior work classifying MNIST \n",
    "numerical digits, this database can be viewed as a multi-faceted data volume \n",
    "explosion: \n",
    "\n",
    "    - a 12 fold increase in datapoints\n",
    "    - a 3.6 fold increase in classes\n",
    "    - a 1.3 fold increase in image size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4caee6c",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### TOC:\n",
    "1. [Data Loading / Cleaning](#s1)\n",
    "2. [Exploratory Data Analysis](#s2)\n",
    "3. [Feature Space Reduction](#s3)\n",
    "4. [Classification / Evaluation](#s4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3536919",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### Data Loading / Cleaning <a class=\"anchor\" id=\"s1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c4539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# consolidated module imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import time\n",
    "from dask import array as da, dataframe as ddf, distributed\n",
    "from joblib import delayed, Parallel\n",
    "from dask_ml.decomposition import TruncatedSVD\n",
    "from PIL import Image, ImageOps\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from subprocess import check_call\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f11c4a",
   "metadata": {
    "cell_marker": "''','''",
    "region_name": "md"
   },
   "source": [
    "The below cell skip re-downloading the .zip file if said zip file and the \n",
    "train / validation folders are present in the current directory. Unzipping the \n",
    "dataset may take a while depending upon computer specs, it is expanding from \n",
    "1.7GB to a little over 3.3GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a18237d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading and unzipping kaggle images\n",
    "paths = ['Train', 'Validation', 'handwritten-characters.zip']\n",
    "checks = [os.path.exists(path) for path in paths]\n",
    "if set(checks) != {True}:\n",
    "    cmd = 'kaggle datasets download -d vaibhao/handwritten-characters'\n",
    "    check_call(cmd, shell = True)\n",
    "    with ZipFile('handwritten-characters.zip', 'r') as z:\n",
    "        z.extractall()\n",
    "    try:\n",
    "        check_call('rm -r dataset', shell = True)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148b3232",
   "metadata": {
    "cell_marker": "''','''",
    "lines_to_next_cell": 1,
    "region_name": "md"
   },
   "source": [
    "The 'to_array' function pads all images that do not match a size of (32, 32) \n",
    "with a 2px border."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7655e2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# functions to load images\n",
    "def to_array(full):\n",
    "    '''\n",
    "    Reads in an image and returns a, padded if necessary, flattened array.\n",
    "\n",
    "        Arguments:\n",
    "            full (string): full string path to image\n",
    "\n",
    "        Returns:\n",
    "            arr (array): flattened 1d array representing image\n",
    "    '''\n",
    "\n",
    "    img = Image.open(full)\n",
    "    if img.size != (32, 32):\n",
    "        img = ImageOps.expand(img, border = 2)\n",
    "    return np.array(img).ravel()\n",
    "\n",
    "def load_from_path(path):\n",
    "    '''\n",
    "    Loads images from directory into numpy array.\n",
    "\n",
    "        Arguments:\n",
    "            path (string): path to directory to be indexed\n",
    "\n",
    "        Returns:\n",
    "            images (array): n x d array of flattened images\n",
    "            labels (array): n x 1 array of labels\n",
    "\n",
    "    '''\n",
    "\n",
    "    path = path + '/' if path[-1] != '/' else path\n",
    "    children = os.listdir(path)\n",
    "    imgs = []\n",
    "    labs = []\n",
    "    for dir in children:\n",
    "        files = os.listdir(path + dir)\n",
    "        imgs.extend(Parallel(n_jobs = -1)(delayed(to_array)(path + dir + '/' \\\n",
    "            + f) for f in files))\n",
    "        labs.extend([dir]*len(files))\n",
    "    images = np.vstack(imgs)\n",
    "    labels = np.array(labs)\n",
    "    return (da.from_array(images, chunks = (15625, 1024)), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8324d18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# timing loading of train data\n",
    "t1 = time.time()\n",
    "X_train, y_train = load_from_path('Train')\n",
    "t2 = time.time()\n",
    "print(f'Execution time: {t2 - t1}')\n",
    "print(f'Images loaded: {X_train.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab42939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same for validation data\n",
    "t1 = time.time()\n",
    "X_val, y_val = load_from_path('Validation')\n",
    "t2 = time.time()\n",
    "print(f'Execution time: {t2 - t1}')\n",
    "print(f'Images loaded: {X_val.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214dfb9d",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### Exploratory Data Analysis <a class=\"anchor\" id=\"s2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100ec8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dask client\n",
    "client = distributed.client._get_global_client() or \\\n",
    "    distributed.Client(processes = False)\n",
    "print(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b749a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing training and test class breakdowns\n",
    "classes, counts = [], []\n",
    "for end, full in zip(['train', 'val'], ['Train', 'Validation']):\n",
    "    exec(f'classes, counts = np.unique(y_{end}, return_counts = True)')\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(10, 10)\n",
    "    ax.bar(classes, counts)\n",
    "    ax.set_title(f'{full} Class Size Distribution')\n",
    "    ax.set_xlabel('Class')\n",
    "    ax.set_ylabel('Number of Samples')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9207ee2",
   "metadata": {
    "cell_marker": "''','''",
    "region_name": "md"
   },
   "source": [
    "In both the training and validation dataframes there look to be rather \n",
    "serious class imbalances, centered mostly on numbers 1 - 9 and excluding 7. \n",
    "Sklearn's SVC classifier has a built in class_weight parameter to combat this, \n",
    "though depending on compute times we may seek to manually prune data points \n",
    "from majority classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0088058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset describe\n",
    "print(pd.DataFrame(X_train[:, :4].compute()).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777cb70c",
   "metadata": {
    "cell_marker": "''','''",
    "region_name": "md"
   },
   "source": [
    "Although the above print is only for the first four columns of the \n",
    "dataframe, it is rather representative of most columns. The dataframe is a \n",
    "sparse matrix with values mostly ranging from 0 to 255, some columns have max \n",
    "values lower than this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac703a18",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "plotting mean character plots\n",
    "indices = lambda x: np.where(y_train == x)\n",
    "mean_row = lambda x: np.mean(x, axis = 0)\n",
    "means = Parallel(n_jobs = -1)(delayed(mean_row)(\n",
    "    X_train[indices(cl)].compute()) for cl in classes)\n",
    "fig, ax = plt.subplots(8, 5)\n",
    "fig.set_size_inches(12, 20)\n",
    "means.append(np.array([0]*1024))\n",
    "for num, arr in enumerate(means):\n",
    "    plt.subplot(8, 5, num + 1)\n",
    "    s = sns.heatmap(arr.reshape((32, 32)), cmap = 'binary_r', cbar = False, \\\n",
    "        xticklabels = [], yticklabels = [])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19142200",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### Feature Space Reduction <class=\"anchor\" id=\"s3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95c0c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ededdef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_json": true,
   "formats": "ipynb,py:percent",
   "main_language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
