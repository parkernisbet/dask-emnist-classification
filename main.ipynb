{
 "cells": [
  {
   "cell_type": "raw",
   "id": "811bfbd3",
   "metadata": {
    "active": "py"
   },
   "source": [
    "!/usr/bin/env python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f495547",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "# Dask EMNIST Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a21bac",
   "metadata": {
    "cell_marker": "''','''",
    "region_name": "md"
   },
   "source": [
    "This notebook explores handwritten character classification using \n",
    "Dask-parallelized gradient boosted decision trees (XGBoost). The dataset was \n",
    "sourced from [Kaggle](https://www.kaggle.com/vaibhao/handwritten-characters) \n",
    "and is a semi-subset of the more well known \n",
    "[Extended MNIST](https://www.nist.gov/itl/products-and-services/emnist-dataset) \n",
    "(EMNIST) database. It includes just north of 850,000 handwritten digits, spread \n",
    "across 39 unique characters: all 26 English alphabet letters (A - Z), 9 real \n",
    "numbers (1 - 9), and 4 special characters (@, #, $, &). Note that this \n",
    "dataset's author merged the two categories 'O' (letter) and '0' (number) to \n",
    "reduce misclassifiations. The images have already been divided into train and \n",
    "validation folders, each containing subdirectories for all of the above \n",
    "mentioned 39 characters. In contrast to our prior work classifying MNIST \n",
    "numerical digits, this database can be viewed as a multi-faceted data volume \n",
    "expansion: \n",
    "\n",
    "    - a 12 fold increase in datapoints\n",
    "    - a 3.6 fold increase in classes\n",
    "    - a 1.3 fold increase in image size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4caee6c",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### Table of Contents:\n",
    "1. [Data Loading / Cleaning](#s1)\n",
    "2. [Exploratory Data Analysis](#s2)\n",
    "3. [Feature Space Reduction](#s3)\n",
    "4. [Classification / Evaluation](#s4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3536919",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### Data Loading / Cleaning <a class=\"anchor\" id=\"s1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c4539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# consolidated module imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import seaborn as sns\n",
    "import time\n",
    "from dask import array as da, distributed\n",
    "from dask_ml.decomposition import IncrementalPCA\n",
    "from dask_ml.naive_bayes import GaussianNB\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "from dask_ml.xgboost import XGBClassifier\n",
    "from joblib import delayed, Parallel, parallel_backend\n",
    "from PIL import Image, ImageOps\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from subprocess import check_call\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e47ba53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5f11c4a",
   "metadata": {
    "cell_marker": "''','''",
    "region_name": "md"
   },
   "source": [
    "The below cell skip re-downloading the .zip file if said zip file and the \n",
    "train / validation folders are present in the current directory. Unzipping the \n",
    "dataset may take a while depending upon computer specs, it is expanding from \n",
    "1.7GB to a little over 3.3GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a18237d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading and unzipping kaggle images\n",
    "paths = ['Train', 'Validation', 'handwritten-characters.zip']\n",
    "checks = [os.path.exists(path) for path in paths]\n",
    "if set(checks) != {True}:\n",
    "    cmd = 'kaggle datasets download -d vaibhao/handwritten-characters'\n",
    "    check_call(cmd, shell = True)\n",
    "    with ZipFile('handwritten-characters.zip', 'r') as z:\n",
    "        z.extractall()\n",
    "    try:\n",
    "        check_call('rm -r dataset', shell = True)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148b3232",
   "metadata": {
    "cell_marker": "''','''",
    "lines_to_next_cell": 1,
    "region_name": "md"
   },
   "source": [
    "The 'to_array' function pads all images that do not match a size of (32, 32) \n",
    "with a 2px border."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7655e2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# functions to load images\n",
    "def to_array(full):\n",
    "    '''\n",
    "    Reads in an image and returns a, padded if necessary, flattened array.\n",
    "\n",
    "        Arguments:\n",
    "            full (string): full string path to image\n",
    "\n",
    "        Returns:\n",
    "            arr (array): flattened 1d array representing image\n",
    "    '''\n",
    "\n",
    "    img = Image.open(full)\n",
    "    if img.size != (32, 32):\n",
    "        img = ImageOps.expand(img, border = 2)\n",
    "    return np.array(img).ravel()\n",
    "\n",
    "def load_from_path(path):\n",
    "    '''\n",
    "    Loads images from directory into dask array.\n",
    "\n",
    "        Arguments:\n",
    "            path (string): path to directory to be indexed\n",
    "\n",
    "        Returns:\n",
    "            images (array): n x d dask array of flattened images\n",
    "            labels (array): n x 1 array of labels\n",
    "\n",
    "    '''\n",
    "\n",
    "    path = path + '/' if path[-1] != '/' else path\n",
    "    children = os.listdir(path)\n",
    "    imgs = []\n",
    "    labs = []\n",
    "    for dir in children:\n",
    "        files = os.listdir(path + dir)\n",
    "        imgs.extend(Parallel(n_jobs = -1)(delayed(to_array)(path + dir + '/' \\\n",
    "            + f) for f in files))\n",
    "        labs.extend([dir]*len(files))\n",
    "    images = np.vstack(imgs)\n",
    "    labels = np.array(labs)\n",
    "    return (da.from_array(images, chunks = (15625, 1024)), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8324d18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# timing loading of train data\n",
    "t1 = time.time()\n",
    "X_train, y_train = load_from_path('Train')\n",
    "t2 = time.time()\n",
    "print(f'Execution time: {t2 - t1}')\n",
    "print(f'Images loaded: {X_train.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab42939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same for validation data\n",
    "t1 = time.time()\n",
    "X_val, y_val = load_from_path('Validation')\n",
    "t2 = time.time()\n",
    "print(f'Execution time: {t2 - t1}')\n",
    "print(f'Images loaded: {X_val.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214dfb9d",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### Exploratory Data Analysis <a class=\"anchor\" id=\"s2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100ec8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dask client\n",
    "client = distributed.client._get_global_client() or \\\n",
    "    distributed.Client(processes = False)\n",
    "print(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b749a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing training and test class breakdowns\n",
    "classes, counts = [], []\n",
    "for end, full in zip(['train', 'val'], ['Train', 'Validation']):\n",
    "    exec(f'classes, counts = np.unique(y_{end}, return_counts = True)')\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(10, 10)\n",
    "    ax.bar(classes, counts)\n",
    "    ax.set_title(f'{full} Class Size Distribution')\n",
    "    ax.set_xlabel('Class')\n",
    "    ax.set_ylabel('Number of Samples')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9207ee2",
   "metadata": {
    "cell_marker": "''','''",
    "region_name": "md"
   },
   "source": [
    "In both the training and validation dataframes there look to be rather \n",
    "serious class imbalances, centered mostly on numbers 1 - 9 and excluding 7. \n",
    "Sklearn's SVC classifier has a built in class_weight parameter to combat this, \n",
    "though depending on compute times we may seek to manually prune data points \n",
    "from majority classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0088058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset describe\n",
    "ind = [random.randint(0, 1023) for x in range(12)]\n",
    "ind.sort()\n",
    "print(pd.DataFrame(X_train[:, np.r_[ind]].compute(), \\\n",
    "    columns = [f'feature {str(x)}' for x in ind]).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777cb70c",
   "metadata": {
    "cell_marker": "''','''",
    "region_name": "md"
   },
   "source": [
    "Although the above print is only for 12 random dataframe columns, it is \n",
    "still rather representative of most other columns. The dataframe is a \"mostly\" \n",
    "sparse matrix with values \"mostly\" ranging from 0 to 255, though there are some \n",
    "columns that run contrary to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e50ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparsity sanity check\n",
    "zeros = (X_train == 0).compute().sum()\n",
    "total = X_train.size\n",
    "print(f'X_train sparseness: {round(100*zeros/total, 2)}%')\n",
    "zeros = (X_val == 0).compute().sum()\n",
    "total = X_val.size\n",
    "print(f'X_val sparseness: {round(100*zeros/total, 2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e951baac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting mean character plots\n",
    "indices = lambda x: np.where(y_train == x)\n",
    "mean_row = lambda x: np.mean(x, axis = 0)\n",
    "means = Parallel(n_jobs = -1)(delayed(mean_row)(\n",
    "    X_train[indices(cl)].compute()) for cl in classes)\n",
    "fig, ax = plt.subplots(8, 5)\n",
    "fig.set_size_inches(12, 20)\n",
    "means.append(np.array([0]*1024))\n",
    "for num, arr in enumerate(means):\n",
    "    plt.subplot(8, 5, num + 1)\n",
    "    s = sns.heatmap(arr.reshape((32, 32)), cmap = 'binary_r', cbar = False, \\\n",
    "        xticklabels = [], yticklabels = [])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19142200",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### Feature Space Reduction <a class=\"anchor\" id=\"s3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341945e6",
   "metadata": {
    "cell_marker": "''','''",
    "region_name": "md"
   },
   "source": [
    "At the moment, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95c0c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling arrays\n",
    "ss = StandardScaler()\n",
    "ss.fit(X_train)\n",
    "X_train = ss.transform(X_train)\n",
    "X_val = ss.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066072d2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# out of memory pca decomposition\n",
    "ipca = IncrementalPCA(n_components = 1024, batch_size = 15625)\n",
    "t1 = time.time()\n",
    "with parallel_backend('dask'):\n",
    "    ipca.fit(X_train)\n",
    "t2 = time.time()\n",
    "print(f'Fit time: {t2 - t1}')\n",
    "ratios = ipca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bde828",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# function to find optimal component count\n",
    "def find_n(ratios, tol):\n",
    "    '''\n",
    "    Finds minimum number of components required to achieve passed tolerance.\n",
    "\n",
    "        Arguments:\n",
    "            ratios (array): svd variance ratios\n",
    "            tol (float): minimum accumulative explained variance\n",
    "\n",
    "        Returns:\n",
    "            n (int): minimum number of components for tolerance\n",
    "    '''\n",
    "\n",
    "    low = 0\n",
    "    high = len(ratios)\n",
    "    while True:\n",
    "        ind = (low + high)//2\n",
    "        if ratios[:ind].sum() > tol:\n",
    "            if high == ind:\n",
    "                break\n",
    "            high = ind\n",
    "        else:\n",
    "            if low == ind:\n",
    "                break\n",
    "            low = ind\n",
    "    return high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf57b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding optimal n component number\n",
    "ns = []\n",
    "for i, tol in enumerate([.95, .99]):\n",
    "    ns.append(find_n(ratios, tol))\n",
    "    print(f'Optimal n, tol = {tol}: {ns[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851d0a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refitting pca model for .95\n",
    "ipca = IncrementalPCA(n_components = ns[0], batch_size = 15625)\n",
    "t1 = time.time()\n",
    "with parallel_backend('dask'):\n",
    "    ipca.fit(X_train)\n",
    "t2 = time.time()\n",
    "print(f'Fit time: {t2 - t1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eae2920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming dask arrays\n",
    "X_train = ipca.transform(X_train).rechunk('auto')\n",
    "X_val = ipca.transform(X_val).rechunk('auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316be7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving data arrays to pickles\n",
    "pickle.dump(X_train, open('X_train.pkl', 'wb'))\n",
    "pickle.dump(y_train, open('y_train.pkl', 'wb'))\n",
    "pickle.dump(X_val, open('X_val.pkl', 'wb'))\n",
    "pickle.dump(y_val, open('y_val.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8e6e5d",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### Classification / Evaluation <a class=\"anchor\" id=\"s4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf0ec6d",
   "metadata": {
    "cell_marker": "''','''",
    "region_name": "md"
   },
   "source": [
    "This section is more of a three-for-one, as it includes construction of the \n",
    "underlying XGBoost classifier, incremental hyperparameter tuning to lock in an \n",
    "optimal configuration, and then finally scoring using the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ededdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding labels as numbers\n",
    "maps = {char: num for num, char in enumerate(classes)}\n",
    "to_num = lambda x: maps[x]\n",
    "y_train = da.from_array(np.array(list(map(to_num, y_train.flatten()))), \\\n",
    "    chunks = (31250))\n",
    "y_val = da.from_array(np.array(list(map(to_num, y_val.flatten()))), \\\n",
    "    chunks = (31250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f614fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training xgboost classifier\n",
    "xgb = XGBClassifier(booster = 'gbtree', objective = 'multi:softprob', \\\n",
    "    max_delta_step = 1, eval_metric = 'auc', seed = 42, max_depth = 6, \\\n",
    "    min_child_weight = 2, subsample = .5, num_parallel_trees = 12, \\\n",
    "    use_label_encoder = False, verbosity = 2, colsample_bytree = .5, \\\n",
    "    n_estimators = 200)\n",
    "# params = {'max_depth': list(range(4, 16)), \\\n",
    "#     'min_child_weight': list(range(1, 20, 2)), \\\n",
    "#     'subsample': np.linspace(.5, 1.0, 10)}\n",
    "# search = HyperbandSearchCV(xgb, params, max_iter = 30, aggressiveness = 4)\n",
    "t1 = time.time()\n",
    "# search.fit(X_train, y_train)\n",
    "xgb.fit(X_train, y_train, classes = np.unique(y_train))\n",
    "t2 = time.time()\n",
    "print(t2 - t1)\n",
    "print(xgb.best_params_)\n",
    "pickle.dump(xgb, open('xgb.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da74f927",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pickle.load(open('y_train.pkl'))\n",
    "y_val = pickle.load(open('y_val.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a26014b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive bayes classifier\n",
    "gnb = GaussianNB()\n",
    "t1 = time.time()\n",
    "gnb.fit(X_train, y_train)\n",
    "t2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e1da40",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gnb.predict(X_val).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcce641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "t1 = time.time()\n",
    "rfc = RandomForestClassifier(n_estimators = 10, max_depth = 12, n_jobs = -1)\n",
    "rfc.fit(X_train, y_train)\n",
    "t2 = time.time()\n",
    "print(t2 - t1, rfc.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fdaf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# closing dask client\n",
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_json": true,
   "formats": "ipynb,py:percent",
   "main_language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
