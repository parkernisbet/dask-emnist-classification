{
 "cells": [
  {
   "cell_type": "raw",
   "id": "811bfbd3",
   "metadata": {
    "active": "py"
   },
   "source": [
    "!/usr/bin/env python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f495547",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "# Dask EMNIST Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a21bac",
   "metadata": {
    "cell_marker": "''','''",
    "region_name": "md"
   },
   "source": [
    "This notebook explores handwritten character classification using \n",
    "Dask-parallelized gradient boosted decision trees (LightGBM). The dataset was \n",
    "sourced from [Kaggle](https://www.kaggle.com/vaibhao/handwritten-characters) \n",
    "and is a semi-subset of the more well known \n",
    "[Extended MNIST](https://www.nist.gov/itl/products-and-services/emnist-dataset) \n",
    "(EMNIST) database. It includes just north of 850,000 handwritten digits, spread \n",
    "across 39 unique characters: all 26 English alphabet letters (A - Z), 9 real \n",
    "numbers (1 - 9), and 4 special characters (@, #, $, &). Note that this \n",
    "dataset's author merged the two categories 'O' (letter) and '0' (number) to \n",
    "reduce misclassifiations. The images have already been divided into train and \n",
    "validation folders, each containing subdirectories for all of the above \n",
    "mentioned 39 characters.\n",
    "\n",
    "Our work won't include the entire image set, but rather only a subset. The \n",
    "full dataset suffers from severe class imbalance, so we will be limiting the \n",
    "loading of images to keep all classes equivalent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4caee6c",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### Table of Contents:\n",
    "1. [Data Loading / Cleaning](#s1)\n",
    "2. [Exploratory Data Analysis](#s2)\n",
    "3. [Feature Space Reduction](#s3)\n",
    "4. [Classification / Evaluation](#s4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3536919",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### Data Loading / Cleaning <a class=\"anchor\" id=\"s1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c4539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# consolidated module imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import seaborn as sns\n",
    "import time\n",
    "from dask import array as da, distributed\n",
    "from dask_ml.decomposition import IncrementalPCA\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from dask_ml.naive_bayes import GaussianNB\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "from hyperopt import fmin, hp, STATUS_OK, tpe, Trials\n",
    "from joblib import delayed, Parallel, parallel_backend\n",
    "from lightgbm import DaskLGBMClassifier\n",
    "from PIL import Image, ImageOps\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from subprocess import check_call\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f11c4a",
   "metadata": {
    "cell_marker": "''','''",
    "region_name": "md"
   },
   "source": [
    "The below cell skip re-downloading the .zip file if said zip file and the \n",
    "train / validation folders are present in the current directory. Unzipping the \n",
    "dataset may take a while depending upon computer specs, it is expanding from \n",
    "1.7GB to a little over 3.3GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a18237d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading and unzipping kaggle images\n",
    "paths = ['Train', 'Validation', 'handwritten-characters.zip']\n",
    "checks = [os.path.exists(path) for path in paths]\n",
    "if set(checks) != {True}:\n",
    "    cmd = 'kaggle datasets download -d vaibhao/handwritten-characters'\n",
    "    check_call(cmd, shell = True)\n",
    "    with ZipFile('handwritten-characters.zip', 'r') as z:\n",
    "        z.extractall()\n",
    "    try:\n",
    "        check_call('rm -r dataset', shell = True)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bac1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking class imbalance\n",
    "counter = lambda x: len(os.listdir(x))\n",
    "dirs = sorted(os.listdir('Train'))\n",
    "train_counts = {}\n",
    "val_counts = {}\n",
    "for dir in dirs:\n",
    "    train_counts[dir] = len(os.listdir(f'Train/{dir}'))\n",
    "    val_counts[dir] = len(os.listdir(f'Validation/{dir}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d48459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing dirstributions\n",
    "for end, full in zip(['train', 'val'], ['Training', 'Validation']):\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(10, 10)\n",
    "    exec(f'ax.bar({end}_counts.keys(), {end}_counts.values())')\n",
    "    ax.set_title(f'{full} Class Distribution')\n",
    "    ax.set_xlabel('Class')\n",
    "    ax.set_ylabel('Number of Samples')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148b3232",
   "metadata": {
    "cell_marker": "''','''",
    "region_name": "md"
   },
   "source": [
    "In both the training and validation directories there look to be rather \n",
    "serious class imbalances, centered mostly on numbers 1 - 9 (excluding 7). \n",
    "Some sklearn modules do have the ability to counteract this with a built in \n",
    "'class_weight' parameter (by inverse weighting majority classes during \n",
    "training), though to be safe we only going to load in enough images so that all \n",
    "classes remain equal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983c2292",
   "metadata": {
    "cell_marker": "''','''",
    "lines_to_next_cell": 1,
    "region_name": "md"
   },
   "source": [
    "The 'to_array' function below pads all images that do not match a size of \n",
    "(32, 32) with a 2px border."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7655e2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# functions to load images\n",
    "def to_array(full):\n",
    "    '''\n",
    "    Reads in an image and returns a, padded if necessary, flattened array.\n",
    "\n",
    "        Arguments:\n",
    "            full (string): full string path to image\n",
    "\n",
    "        Returns:\n",
    "            arr (array): flattened 1d array representing image\n",
    "    '''\n",
    "\n",
    "    img = Image.open(full)\n",
    "    if img.size != (32, 32):\n",
    "        img = ImageOps.expand(img, border = 2)\n",
    "    return np.array(img).ravel()\n",
    "\n",
    "def load_from_path(path):\n",
    "    '''\n",
    "    Loads images from directory into dask array.\n",
    "\n",
    "        Arguments:\n",
    "            path (string): path to directory to be indexed\n",
    "\n",
    "        Returns:\n",
    "            images (array): n x d dask array of flattened images\n",
    "            labels (array): n x 1 array of labels\n",
    "\n",
    "    '''\n",
    "\n",
    "    limit = min(train_counts.values()) if path == 'Train' else \\\n",
    "        min(val_counts.values())\n",
    "    path = path + '/' if path[-1] != '/' else path\n",
    "    children = os.listdir(path)\n",
    "    imgs = []\n",
    "    labs = []\n",
    "    for dir in children:\n",
    "        files = os.listdir(path + dir)\n",
    "        files = random.sample(files, limit)\n",
    "        imgs.extend(Parallel(n_jobs = -1)(delayed(to_array)(path + dir + '/' \\\n",
    "            + f) for f in files))\n",
    "        labs.extend([dir]*len(files))\n",
    "    images = np.vstack(imgs)\n",
    "    labels = np.array(labs)\n",
    "    return (da.from_array(images, chunks = (15625, 1024)), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8324d18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# timing loading of train data\n",
    "t1 = time.time()\n",
    "X_train, y_train = load_from_path('Train')\n",
    "t2 = time.time()\n",
    "print(f'Execution time: {t2 - t1}')\n",
    "print(f'Images loaded: {X_train.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab42939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same for validation data\n",
    "t1 = time.time()\n",
    "X_val, y_val = load_from_path('Validation')\n",
    "t2 = time.time()\n",
    "print(f'Execution time: {t2 - t1}')\n",
    "print(f'Images loaded: {X_val.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea4af09",
   "metadata": {
    "cell_marker": "''','''",
    "region_name": "md"
   },
   "source": [
    "In total we will be working with 170820 images, quite a bit less than the \n",
    "original dataset. Outside of this project, something to explore here would be \n",
    "to try to generate more images from the original set, albeit with minor \n",
    "transformations to give the illusion of uniqueness (shifts, skews, scales, \n",
    "etc.). This would allow for us to take advantage of a larger portion of the \n",
    "initial image set, though this could lead to overfitting in the long run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11eb29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train.compute(), \\\n",
    "    y_train, shuffle = True, test_size = .15)\n",
    "X_train = da.from_array(X_train, chunks = (15625, 1024))\n",
    "X_test = da.from_array(X_test, chunks = (15625, 1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214dfb9d",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### Exploratory Data Analysis <a class=\"anchor\" id=\"s2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100ec8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dask client\n",
    "client = distributed.client._get_global_client() or \\\n",
    "    distributed.Client(n_workers = 2, processes = False)\n",
    "print(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0088058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset describe\n",
    "ind = [random.randint(0, 1023) for x in range(12)]\n",
    "ind.sort()\n",
    "print(pd.DataFrame(X_train[:, np.r_[ind]].compute(), \\\n",
    "    columns = [f'feature {str(x)}' for x in ind]).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777cb70c",
   "metadata": {
    "cell_marker": "''','''",
    "region_name": "md"
   },
   "source": [
    "Although the above print is only for 12 random array columns, it is \n",
    "still rather representative of most other columns. The array is a \"mostly\" \n",
    "sparse matrix with values \"mostly\" ranging from 0 to 255, though there are some \n",
    "columns that run contrary to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e50ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparsity sanity check\n",
    "zeros = (X_train == 0).compute().sum()\n",
    "total = X_train.size\n",
    "print(f'X_train sparseness: {round(100*zeros/total, 2)}%')\n",
    "zeros = (X_val == 0).compute().sum()\n",
    "total = X_val.size\n",
    "print(f'X_val sparseness: {round(100*zeros/total, 2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e951baac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting mean character plots\n",
    "classes = train_counts.keys()\n",
    "indices = lambda x: np.where(y_train == x)\n",
    "mean_row = lambda x: np.mean(x, axis = 0)\n",
    "means = Parallel(n_jobs = -1)(delayed(mean_row)(\n",
    "    X_train[indices(cl)].compute()) for cl in classes)\n",
    "fig, ax = plt.subplots(8, 5)\n",
    "fig.set_size_inches(12, 20)\n",
    "means.append(np.array([0]*1024))\n",
    "for num, arr in enumerate(means):\n",
    "    plt.subplot(8, 5, num + 1)\n",
    "    s = sns.heatmap(arr.reshape((32, 32)), cmap = 'binary_r', cbar = False, \\\n",
    "        xticklabels = [], yticklabels = [])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161aeca9",
   "metadata": {
    "cell_marker": "''','''",
    "region_name": "md"
   },
   "source": [
    "Note that some letters are a combination of both lower and upper case \n",
    "characters. This should make for some added complexity when trying to predict \n",
    "from our constructed models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19142200",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### Feature Space Reduction <a class=\"anchor\" id=\"s3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341945e6",
   "metadata": {
    "cell_marker": "''','''",
    "region_name": "md"
   },
   "source": [
    "Normally there are quite a few methods we could use from sklearn to reduce \n",
    "our total features, though not all of these are easily parallelizable \n",
    "with Dask. Lucky for us, dask_ml includes a pre-built version of sklearn's \n",
    "'IncrementalPCA' module that plays nice with Dask's backend. The only caveat is \n",
    "that the dask_ml implementation doesn't completely scale the input data (only a \n",
    "mean centering), so we will be scaling it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95c0c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling arrays\n",
    "ss = StandardScaler()\n",
    "ss.fit(X_train)\n",
    "X_train = ss.transform(X_train)\n",
    "X_test = ss.transform(X_test)\n",
    "X_val = ss.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066072d2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# out of memory pca decomposition\n",
    "ipca = IncrementalPCA(n_components = 1024, batch_size = 15625)\n",
    "t1 = time.time()\n",
    "with parallel_backend('dask'):\n",
    "    ipca.fit(X_train)\n",
    "t2 = time.time()\n",
    "print(f'Fit time: {t2 - t1}')\n",
    "ratios = ipca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bde828",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# function to find optimal component count\n",
    "def find_n(ratios, tol):\n",
    "    '''\n",
    "    Finds minimum number of components required to achieve passed tolerance.\n",
    "\n",
    "        Arguments:\n",
    "            ratios (array): svd variance ratios\n",
    "            tol (float): minimum accumulative explained variance\n",
    "\n",
    "        Returns:\n",
    "            n (int): minimum number of components for tolerance\n",
    "    '''\n",
    "\n",
    "    low = 0\n",
    "    high = len(ratios)\n",
    "    while True:\n",
    "        ind = (low + high)//2\n",
    "        if ratios[:ind].sum() > tol:\n",
    "            if high == ind:\n",
    "                break\n",
    "            high = ind\n",
    "        else:\n",
    "            if low == ind:\n",
    "                break\n",
    "            low = ind\n",
    "    return high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf57b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding optimal n component number\n",
    "ns = []\n",
    "for i, tol in enumerate([.95, .99]):\n",
    "    ns.append(find_n(ratios, tol))\n",
    "    print(f'Optimal n, tol = {tol}: {ns[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4928824",
   "metadata": {
    "cell_marker": "''','''",
    "region_name": "md"
   },
   "source": [
    "PCA allows for a more than 50% reduction in feature count, while only \n",
    "incurring a 5% loss in explained variance. For datasets orders of magnitude \n",
    "larger than ours, this would have critical impacts on training times and \n",
    "storage size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851d0a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refitting pca model for .95\n",
    "ipca = IncrementalPCA(n_components = ns[0], batch_size = 15625)\n",
    "t1 = time.time()\n",
    "with parallel_backend('dask'):\n",
    "    ipca.fit(X_train)\n",
    "t2 = time.time()\n",
    "print(f'Fit time: {t2 - t1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eae2920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reducing dask arrays\n",
    "X_train = ipca.transform(X_train).rechunk('auto')\n",
    "X_test = ipca.transform(X_test).rechunk('auto')\n",
    "X_val = ipca.transform(X_val).rechunk('auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316be7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving data arrays to pickles\n",
    "pickle.dump(X_train, open('X_train.pkl', 'wb'))\n",
    "pickle.dump(y_train, open('y_train.pkl', 'wb'))\n",
    "pickle.dump(X_test, open('X_test.pkl', 'wb'))\n",
    "pickle.dump(y_test, open('y_test.pkl', 'wb'))\n",
    "pickle.dump(X_val, open('X_val.pkl', 'wb'))\n",
    "pickle.dump(y_val, open('y_val.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8e6e5d",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "### Classification / Evaluation <a class=\"anchor\" id=\"s4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf0ec6d",
   "metadata": {
    "cell_marker": "''','''",
    "region_name": "md"
   },
   "source": [
    "This section is more of a three-for-one, as it includes classifier \n",
    "construction, hyperparameter tuning to lock in an optimal configuration, and \n",
    "test set scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ededdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading variables from disk\n",
    "for var in ['X_train', 'y_train', 'X_val', 'y_val']:\n",
    "    exec(f'{var} = pickle.load(open(\"{var}.pkl\", \"rb\"))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f614fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model accuracies\n",
    "accs = {}\n",
    "times = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bced1b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive bayes classifier\n",
    "gnb = GaussianNB()\n",
    "t1 = time.time()\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred = gnb.predict(X_test).compute()\n",
    "accs['gnb'] = (f1_score(y_test, y_pred, average = 'macro'), \\\n",
    "    accuracy_score(y_test, y_pred))\n",
    "t2 = time.time()\n",
    "times['gnb'] = t2 - t1\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44415cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The accuracy for the above model is quite low, but this can be (at least \n",
    "partially) attributed to limited samples from each class. Not to worry though, \n",
    "this model was added as a quick-training baseline to contrast with our lightgbm \n",
    "model.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dea5b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightgbm tuning\n",
    "# def lgb_loss(params):\n",
    "#     lgb = DaskLGBMClassifier(**params, n_estimators = 50)\n",
    "#     lgb.fit(X_train, da.from_array(y_train, chunks = 31250))\n",
    "#     y_pred = lgb.predict(X_val).compute()\n",
    "#     score = f1_score(y_val, y_pred, average = 'macro')\n",
    "#     return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "# space = {'max_depth': 4 + hp.randint('max_depth', 4),\n",
    "#     }\n",
    "# trials = Trials()\n",
    "# best = fmin(fn = lgb_loss, space = space, algo = tpe.suggest, max_evals = 4, \\\n",
    "#     trials = trials)\n",
    "# print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2b96cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightgbm classifier\n",
    "t1 = time.time()\n",
    "dlgb = DaskLGBMClassifier(max_depth = 8, tree_learner = 'data', \\\n",
    "    n_estimators = 45)\n",
    "dlgb.fit(X_train, da.from_array(y_train, chunks = 31250))\n",
    "y_pred = dlgb.predict(X_test).compute()\n",
    "accs['lgb'] = (f1_score(y_test, y_pred, average = 'macro'), \\\n",
    "    accuracy_score(y_test, y_pred))\n",
    "t2 = time.time()\n",
    "times['gnb'] = t2 - t1\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7397ae7d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fdaf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# closing dask client\n",
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_json": true,
   "formats": "ipynb,py:percent",
   "main_language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
